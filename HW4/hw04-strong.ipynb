{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6786c2b0e2614ad389620246cb2178f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d592098920140dab61aac5410568c36","IPY_MODEL_da681e3cc353420cb142d56df0fce231","IPY_MODEL_401ae722b95c4ff59b836422dbe71edc"],"layout":"IPY_MODEL_4efbfb7c7cb54276862e5321209d57fa"}},"0d592098920140dab61aac5410568c36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6dcb3ec9c624171966bb889808dbcb3","placeholder":"â€‹","style":"IPY_MODEL_100abf072991474abfcee871d2b83f37","value":"100%"}},"da681e3cc353420cb142d56df0fce231":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_decc12da6f5742ec8b7ec7789ee434ab","max":8000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0117ac88c98440b29ab1f452107cbe1a","value":8000}},"401ae722b95c4ff59b836422dbe71edc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d64942a0eaa409a93df049bb594c062","placeholder":"â€‹","style":"IPY_MODEL_8c8721e504cf434eb9e263fb9983969a","value":" 8000/8000 [00:33&lt;00:00, 256.07it/s]"}},"4efbfb7c7cb54276862e5321209d57fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6dcb3ec9c624171966bb889808dbcb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"100abf072991474abfcee871d2b83f37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"decc12da6f5742ec8b7ec7789ee434ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0117ac88c98440b29ab1f452107cbe1a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d64942a0eaa409a93df049bb594c062":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c8721e504cf434eb9e263fb9983969a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task description\n- Classify the speakers of given features.\n- Main goal: Learn how to use transformer.\n- Baselines:\n  - Easy: Run sample code and know how to use transformer.\n  - Medium: Know how to adjust parameters of transformer.\n  - Strong: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer.\n  - Boss: Implement [Self-Attention Pooling](https://arxiv.org/pdf/2008.01077v1.pdf) & [Additive Margin Softmax](https://arxiv.org/pdf/1801.05599.pdf) to further boost the performance.\n\n- Other links\n  - Competiton: [link](https://www.kaggle.com/t/49ea0c385a974db5919ec67299ba2e6b)\n  - Slide: [link](https://docs.google.com/presentation/d/1LDAW0GGrC9B6D7dlNdYzQL6D60-iKgFr/edit?usp=sharing&ouid=104280564485377739218&rtpof=true&sd=true)\n  - Data: [link](https://github.com/googly-mingto/ML2023HW4/releases)\n\n","metadata":{"id":"C_jdZ5vHJ4A9"}},{"cell_type":"code","source":"!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partaa\n!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partab\n!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partac\n!wget https://github.com/googly-mingto/ML2023HW4/releases/download/data/Dataset.tar.gz.partad\n\n!cat Dataset.tar.gz.part* > Dataset.tar.gz\n!rm Dataset.tar.gz.partaa\n!rm Dataset.tar.gz.partab\n!rm Dataset.tar.gz.partac\n!rm Dataset.tar.gz.partad\n# unzip the file\n!tar zxf Dataset.tar.gz\n!rm Dataset.tar.gz","metadata":{"id":"gtKxUzSgXKj3","outputId":"3f59402c-95a7-4fbd-a39c-57606590a89c","execution":{"iopub.status.busy":"2024-06-29T17:52:05.963535Z","iopub.execute_input":"2024-06-29T17:52:05.963880Z","iopub.status.idle":"2024-06-29T17:56:51.337032Z","shell.execute_reply.started":"2024-06-29T17:52:05.963850Z","shell.execute_reply":"2024-06-29T17:56:51.335738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar zxf Dataset.tar.gz","metadata":{"id":"U6Y1cfpDfpON","outputId":"6ba26637-5c7b-48a9-be0b-1f10ba76590a","execution":{"iopub.status.busy":"2024-06-29T17:56:51.339277Z","iopub.execute_input":"2024-06-29T17:56:51.339592Z","iopub.status.idle":"2024-06-29T17:56:52.290634Z","shell.execute_reply.started":"2024-06-29T17:56:51.339562Z","shell.execute_reply":"2024-06-29T17:56:52.289768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport random\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nset_seed(87)","metadata":{"id":"E6burzCXIyuA","execution":{"iopub.status.busy":"2024-06-29T17:56:52.292211Z","iopub.execute_input":"2024-06-29T17:56:52.292594Z","iopub.status.idle":"2024-06-29T17:56:55.743570Z","shell.execute_reply.started":"2024-06-29T17:56:52.292558Z","shell.execute_reply":"2024-06-29T17:56:55.742712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data\n\n## Dataset\n- Original dataset is [Voxceleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html).\n- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb2.\n- We randomly select 600 speakers from Voxceleb2.\n- Then preprocess the raw waveforms into mel-spectrograms.\n\n- Args:\n  - data_dir: The path to the data directory.\n  - metadata_path: The path to the metadata.\n  - segment_len: The length of audio segment for training.\n- The architecture of data directory \\\\\n  - data directory \\\\\n  |---- metadata.json \\\\\n  |---- testdata.json \\\\\n  |---- mapping.json \\\\\n  |---- uttr-{random string}.pt \\\\\n\n- The information in metadata\n  - \"n_mels\": The dimention of mel-spectrogram.\n  - \"speakers\": A dictionary.\n    - Key: speaker ids.\n    - value: \"feature_path\" and \"mel_len\"\n\n\nFor efficiency, we segment the mel-spectrograms into segments in the traing step.","metadata":{"id":"k7dVbxW2LASN"}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport random\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nclass myDataset(Dataset):\n\tdef __init__(self, data_dir, segment_len=128):\n\t\tself.data_dir = data_dir\n\t\tself.segment_len = segment_len\n\n\t\t# Load the mapping from speaker neme to their corresponding id.\n\t\tmapping_path = Path(data_dir) / \"mapping.json\"\n\t\tmapping = json.load(mapping_path.open())\n\t\tself.speaker2id = mapping[\"speaker2id\"]\n\n\t\t# Load metadata of training data.\n\t\tmetadata_path = Path(data_dir) / \"metadata.json\"\n\t\tmetadata = json.load(open(metadata_path))[\"speakers\"]\n\n\t\t# Get the total number of speaker.\n\t\tself.speaker_num = len(metadata.keys())\n\t\tself.data = []\n\t\tfor speaker in metadata.keys():\n\t\t\tfor utterances in metadata[speaker]:\n\t\t\t\tself.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n\n\tdef __len__(self):\n\t\t\treturn len(self.data)\n\n\tdef __getitem__(self, index):\n\t\tfeat_path, speaker = self.data[index]\n\t\t# Load preprocessed mel-spectrogram.\n\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n\n\t\t# Segmemt mel-spectrogram into \"segment_len\" frames.\n\t\tif len(mel) > self.segment_len:\n\t\t\t# Randomly get the starting point of the segment.\n\t\t\tstart = random.randint(0, len(mel) - self.segment_len)\n\t\t\t# Get a segment with \"segment_len\" frames.\n\t\t\tmel = torch.FloatTensor(mel[start:start+self.segment_len])\n\t\telse:\n\t\t\tmel = torch.FloatTensor(mel)\n\t\t# Turn the speaker id into long for computing loss later.\n\t\tspeaker = torch.FloatTensor([speaker]).long()\n\t\treturn mel, speaker\n\n\tdef get_speaker_number(self):\n\t\treturn self.speaker_num","metadata":{"id":"KpuGxl4CI2pr","execution":{"iopub.status.busy":"2024-06-29T17:59:30.159999Z","iopub.execute_input":"2024-06-29T17:59:30.160546Z","iopub.status.idle":"2024-06-29T17:59:30.172395Z","shell.execute_reply.started":"2024-06-29T17:59:30.160513Z","shell.execute_reply":"2024-06-29T17:59:30.171523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataloader\n- Split dataset into training dataset(90%) and validation dataset(10%).\n- Create dataloader to iterate the data.","metadata":{"id":"668hverTMlGN"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\n\n\ndef collate_batch(batch):\n\t# Process features within a batch.\n\t\"\"\"Collate a batch of data.\"\"\"\n\tmel, speaker = zip(*batch)\n\t# Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n\tmel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n\t# mel: (batch size, length, 40)\n\treturn mel, torch.FloatTensor(speaker).long()\n\n\ndef get_dataloader(data_dir, batch_size, n_workers):\n\t\"\"\"Generate dataloader\"\"\"\n\tdataset = myDataset(data_dir)\n\tspeaker_num = dataset.get_speaker_number()\n\t# Split dataset into training dataset and validation dataset\n\ttrainlen = int(0.9 * len(dataset))\n\tlengths = [trainlen, len(dataset) - trainlen]\n\ttrainset, validset = random_split(dataset, lengths)\n\n\ttrain_loader = DataLoader(\n\t\ttrainset,\n\t\tbatch_size=batch_size,\n\t\tshuffle=True,\n\t\tdrop_last=True,\n\t\tnum_workers=n_workers,\n\t\tpin_memory=True,\n\t\tcollate_fn=collate_batch,\n\t)\n\tvalid_loader = DataLoader(\n\t\tvalidset,\n\t\tbatch_size=batch_size,\n\t\tnum_workers=n_workers,\n\t\tdrop_last=True,\n\t\tpin_memory=True,\n\t\tcollate_fn=collate_batch,\n\t)\n\n\treturn train_loader, valid_loader, speaker_num","metadata":{"id":"B7c2gZYoJDRS","execution":{"iopub.status.busy":"2024-06-29T17:59:33.567710Z","iopub.execute_input":"2024-06-29T17:59:33.568066Z","iopub.status.idle":"2024-06-29T17:59:33.577546Z","shell.execute_reply.started":"2024-06-29T17:59:33.568038Z","shell.execute_reply":"2024-06-29T17:59:33.576520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- TransformerEncoderLayer:\n  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n  - Parameters:\n    - d_model: the number of expected features of the input (required).\n\n    - nhead: the number of heads of the multiheadattention models (required).\n\n    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n\n    - dropout: the dropout value (default=0.1).\n\n    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n\n- TransformerEncoder:\n  - TransformerEncoder is a stack of N transformer encoder layers\n  - Parameters:\n    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n\n    - num_layers: the number of sub-encoder-layers in the encoder (required).\n\n    - norm: the layer normalization component (optional).","metadata":{"id":"5FOSZYxrMqhc"}},{"cell_type":"code","source":"!pip install conformer","metadata":{"execution":{"iopub.status.busy":"2024-06-29T17:59:39.069175Z","iopub.execute_input":"2024-06-29T17:59:39.069554Z","iopub.status.idle":"2024-06-29T17:59:52.729781Z","shell.execute_reply.started":"2024-06-29T17:59:39.069523Z","shell.execute_reply":"2024-06-29T17:59:52.728706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom conformer import ConformerBlock","metadata":{"execution":{"iopub.status.busy":"2024-06-29T18:05:12.376350Z","iopub.execute_input":"2024-06-29T18:05:12.377235Z","iopub.status.idle":"2024-06-29T18:05:53.858796Z","shell.execute_reply.started":"2024-06-29T18:05:12.377192Z","shell.execute_reply":"2024-06-29T18:05:53.857970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n        super().__init__()\n        # Project the dimension of features from that of input into d_model.\n        self.prenet = nn.Linear(40, d_model)\n        \n        self.block1 = ConformerBlock(\n                    dim = d_model,\n                    dim_head = 32,\n                    heads = 8,\n                    ff_mult = 4,\n                    conv_expansion_factor = 2,\n                    conv_kernel_size = 31,\n                    attn_dropout = 0.,\n                    ff_dropout = 0.,\n                    conv_dropout = 0.\n                    )\n        \n        self.block2 = ConformerBlock(\n                    dim = d_model,\n                    dim_head = 32,\n                    heads = 8,\n                    ff_mult = 4,\n                    conv_expansion_factor = 2,\n                    conv_kernel_size = 31,\n                    attn_dropout = 0.,\n                    ff_dropout = 0.,\n                    conv_dropout = 0.\n                    )\n        \n        self.block3 = ConformerBlock(\n                    dim = d_model,\n                    dim_head = 32,\n                    heads = 8,\n                    ff_mult = 4,\n                    conv_expansion_factor = 2,\n                    conv_kernel_size = 31,\n                    attn_dropout = 0.1,\n                    ff_dropout = 0.1,\n                    conv_dropout = 0.1\n                    )\n        \n        # Project the the dimension of features from d_model into speaker nums.\n        self.pred_layer = nn.Sequential(\n            nn.Linear(d_model, n_spks),\n        )\n\n    def forward(self, mels):\n        \"\"\"\n        args:\n            mels: (batch size, length, 40)\n        return:\n            out: (batch size, n_spks)\n        \"\"\"\n        # out: (batch size, length, d_model)\n        out = self.prenet(mels)\n        # The encoder layer\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        # mean pooling\n        stats = out.mean(dim=1)\n        # out: (batch, n_spks)\n        out = self.pred_layer(stats)\n        return out","metadata":{"id":"iXZ5B0EKJGs8","execution":{"iopub.status.busy":"2024-06-29T18:06:00.303245Z","iopub.execute_input":"2024-06-29T18:06:00.303740Z","iopub.status.idle":"2024-06-29T18:06:00.314682Z","shell.execute_reply.started":"2024-06-29T18:06:00.303708Z","shell.execute_reply":"2024-06-29T18:06:00.313732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning rate schedule\n- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n- The warmup schedule\n  - Set learning rate to 0 in the beginning.\n  - The learning rate increases linearly from 0 to initial learning rate during warmup period.","metadata":{"id":"W7yX8JinM5Ly"}},{"cell_type":"code","source":"import math\n\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\ndef get_cosine_schedule_with_warmup(\n\toptimizer: Optimizer,\n\tnum_warmup_steps: int,\n\tnum_training_steps: int,\n\tnum_cycles: float = 0.5,\n\tlast_epoch: int = -1,\n):\n\t\"\"\"\n\tCreate a schedule with a learning rate that decreases following the values of the cosine function between the\n\tinitial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n\tinitial lr set in the optimizer.\n\n\tArgs:\n\t\toptimizer (:class:`~torch.optim.Optimizer`):\n\t\tThe optimizer for which to schedule the learning rate.\n\t\tnum_warmup_steps (:obj:`int`):\n\t\tThe number of steps for the warmup phase.\n\t\tnum_training_steps (:obj:`int`):\n\t\tThe total number of training steps.\n\t\tnum_cycles (:obj:`float`, `optional`, defaults to 0.5):\n\t\tThe number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n\t\tfollowing a half-cosine).\n\t\tlast_epoch (:obj:`int`, `optional`, defaults to -1):\n\t\tThe index of the last epoch when resuming training.\n\n\tReturn:\n\t\t:obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n\t\"\"\"\n\tdef lr_lambda(current_step):\n\t\t# Warmup\n\t\tif current_step < num_warmup_steps:\n\t\t\treturn float(current_step) / float(max(1, num_warmup_steps))\n\t\t# decadence\n\t\tprogress = float(current_step - num_warmup_steps) / float(\n\t\t\tmax(1, num_training_steps - num_warmup_steps)\n\t\t)\n\t\treturn max(\n\t\t\t0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n\t\t)\n\n\treturn LambdaLR(optimizer, lr_lambda, last_epoch)","metadata":{"id":"ykt0N1nVJJi2","execution":{"iopub.status.busy":"2024-06-29T18:06:04.138017Z","iopub.execute_input":"2024-06-29T18:06:04.138901Z","iopub.status.idle":"2024-06-29T18:06:04.147236Z","shell.execute_reply.started":"2024-06-29T18:06:04.138863Z","shell.execute_reply":"2024-06-29T18:06:04.146244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Function\n- Model forward function.","metadata":{"id":"-LN2XkteM_uH"}},{"cell_type":"code","source":"import torch\n\n\ndef model_fn(batch, model, criterion, device):\n\t\"\"\"Forward a batch through the model.\"\"\"\n\n\tmels, labels = batch\n\tmels = mels.to(device)\n\tlabels = labels.to(device)\n\n\touts = model(mels)\n\n\tloss = criterion(outs, labels)\n\n\t# Get the speaker id with highest probability.\n\tpreds = outs.argmax(1)\n\t# Compute accuracy.\n\taccuracy = torch.mean((preds == labels).float())\n\n\treturn loss, accuracy","metadata":{"id":"N-rr8529JMz0","execution":{"iopub.status.busy":"2024-06-29T18:06:08.596749Z","iopub.execute_input":"2024-06-29T18:06:08.597458Z","iopub.status.idle":"2024-06-29T18:06:08.603471Z","shell.execute_reply.started":"2024-06-29T18:06:08.597397Z","shell.execute_reply":"2024-06-29T18:06:08.602473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate\n- Calculate accuracy of the validation set.","metadata":{"id":"cwM_xyOtNCI2"}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\n\ndef valid(dataloader, model, criterion, device):\n\t\"\"\"Validate on validation set.\"\"\"\n\n\tmodel.eval()\n\trunning_loss = 0.0\n\trunning_accuracy = 0.0\n\tpbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n\n\tfor i, batch in enumerate(dataloader):\n\t\twith torch.no_grad():\n\t\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n\t\t\trunning_loss += loss.item()\n\t\t\trunning_accuracy += accuracy.item()\n\n\t\tpbar.update(dataloader.batch_size)\n\t\tpbar.set_postfix(\n\t\t\tloss=f\"{running_loss / (i+1):.2f}\",\n\t\t\taccuracy=f\"{running_accuracy / (i+1):.2f}\",\n\t\t)\n\n\tpbar.close()\n\tmodel.train()\n\n\treturn running_accuracy / len(dataloader)","metadata":{"id":"YAiv6kpdJRTJ","execution":{"iopub.status.busy":"2024-06-29T18:06:11.692632Z","iopub.execute_input":"2024-06-29T18:06:11.693273Z","iopub.status.idle":"2024-06-29T18:06:11.700726Z","shell.execute_reply.started":"2024-06-29T18:06:11.693241Z","shell.execute_reply":"2024-06-29T18:06:11.699755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main function","metadata":{"id":"g6ne9G-eNEdG"}},{"cell_type":"code","source":"from tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, random_split\n\n\ndef parse_args():\n\t\"\"\"arguments\"\"\"\n\tconfig = {\n\t\t\"data_dir\": \"./Dataset\",\n\t\t\"save_path\": \"model.ckpt\",\n\t\t\"batch_size\": 64,\n\t\t\"n_workers\": 8,\n\t\t\"valid_steps\": 2000,\n\t\t\"warmup_steps\": 1000,\n\t\t\"save_steps\": 10000,\n\t\t\"total_steps\": 100000,\n\t}\n\n\treturn config\n\n\ndef main(\n\tdata_dir,\n\tsave_path,\n\tbatch_size,\n\tn_workers,\n\tvalid_steps,\n\twarmup_steps,\n\ttotal_steps,\n\tsave_steps,\n):\n\t\"\"\"Main function.\"\"\"\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tprint(f\"[Info]: Use {device} now!\")\n\n\ttrain_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n\ttrain_iterator = iter(train_loader)\n\tprint(f\"[Info]: Finish loading data!\",flush = True)\n\n\tmodel = Classifier(n_spks=speaker_num).to(device)\n\tcriterion = nn.CrossEntropyLoss()\n\toptimizer = AdamW(model.parameters(), lr=1e-3)\n\tscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\tprint(f\"[Info]: Finish creating model!\",flush = True)\n\n\tbest_accuracy = -1.0\n\tbest_state_dict = None\n\n\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n\tfor step in range(total_steps):\n\t\t# Get data\n\t\ttry:\n\t\t\tbatch = next(train_iterator)\n\t\texcept StopIteration:\n\t\t\ttrain_iterator = iter(train_loader)\n\t\t\tbatch = next(train_iterator)\n\n\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n\t\tbatch_loss = loss.item()\n\t\tbatch_accuracy = accuracy.item()\n\n\t\t# Updata model\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tscheduler.step()\n\t\toptimizer.zero_grad()\n\n\t\t# Log\n\t\tpbar.update()\n\t\tpbar.set_postfix(\n\t\t\tloss=f\"{batch_loss:.2f}\",\n\t\t\taccuracy=f\"{batch_accuracy:.2f}\",\n\t\t\tstep=step + 1,\n\t\t)\n\n\t\t# Do validation\n\t\tif (step + 1) % valid_steps == 0:\n\t\t\tpbar.close()\n\n\t\t\tvalid_accuracy = valid(valid_loader, model, criterion, device)\n\n\t\t\t# keep the best model\n\t\t\tif valid_accuracy > best_accuracy:\n\t\t\t\tbest_accuracy = valid_accuracy\n\t\t\t\tbest_state_dict = model.state_dict()\n\n\t\t\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n\n\t\t# Save the best model so far.\n\t\tif (step + 1) % save_steps == 0 and best_state_dict is not None:\n\t\t\ttorch.save(best_state_dict, save_path)\n\t\t\tpbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n\n\tpbar.close()\n\n\nif __name__ == \"__main__\":\n\tmain(**parse_args())","metadata":{"id":"Usv9s-CuJSG7","outputId":"f4f6a983-3559-4f36-efae-402bbf790473","execution":{"iopub.status.busy":"2024-06-29T18:06:14.288603Z","iopub.execute_input":"2024-06-29T18:06:14.289231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\n\n## Dataset of inference","metadata":{"id":"NLatBYAhNNMx"}},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\n\n\nclass InferenceDataset(Dataset):\n\tdef __init__(self, data_dir):\n\t\ttestdata_path = Path(data_dir) / \"testdata.json\"\n\t\tmetadata = json.load(testdata_path.open())\n\t\tself.data_dir = data_dir\n\t\tself.data = metadata[\"utterances\"]\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n\n\tdef __getitem__(self, index):\n\t\tutterance = self.data[index]\n\t\tfeat_path = utterance[\"feature_path\"]\n\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n\n\t\treturn feat_path, mel\n\n\ndef inference_collate_batch(batch):\n\t\"\"\"Collate a batch of data.\"\"\"\n\tfeat_paths, mels = zip(*batch)\n\n\treturn feat_paths, torch.stack(mels)","metadata":{"id":"efS4pCmAJXJH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main funcrion of Inference","metadata":{"id":"tl0WnYwxNK_S"}},{"cell_type":"code","source":"import json\nimport csv\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\n\nimport torch\nfrom torch.utils.data import DataLoader\n\ndef parse_args():\n\t\"\"\"arguments\"\"\"\n\tconfig = {\n\t\t\"data_dir\": \"./Dataset\",\n\t\t\"model_path\": \"./model.ckpt\",\n\t\t\"output_path\": \"./output.csv\",\n\t}\n\n\treturn config\n\n\ndef main(\n\tdata_dir,\n\tmodel_path,\n\toutput_path,\n):\n\t\"\"\"Main function.\"\"\"\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tprint(f\"[Info]: Use {device} now!\")\n\n\tmapping_path = Path(data_dir) / \"mapping.json\"\n\tmapping = json.load(mapping_path.open())\n\n\tdataset = InferenceDataset(data_dir)\n\tdataloader = DataLoader(\n\t\tdataset,\n\t\tbatch_size=1,\n\t\tshuffle=False,\n\t\tdrop_last=False,\n\t\tnum_workers=8,\n\t\tcollate_fn=inference_collate_batch,\n\t)\n\tprint(f\"[Info]: Finish loading data!\",flush = True)\n\n\tspeaker_num = len(mapping[\"id2speaker\"])\n\tmodel = Classifier(n_spks=speaker_num).to(device)\n\tmodel.load_state_dict(torch.load(model_path))\n\tmodel.eval()\n\tprint(f\"[Info]: Finish creating model!\",flush = True)\n\n\tresults = [[\"Id\", \"Category\"]]\n\tfor feat_paths, mels in tqdm(dataloader):\n\t\twith torch.no_grad():\n\t\t\tmels = mels.to(device)\n\t\t\touts = model(mels)\n\t\t\tpreds = outs.argmax(1).cpu().numpy()\n\t\t\tfor feat_path, pred in zip(feat_paths, preds):\n\t\t\t\tresults.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n\n\twith open(output_path, 'w', newline='') as csvfile:\n\t\twriter = csv.writer(csvfile)\n\t\twriter.writerows(results)\n\n\nif __name__ == \"__main__\":\n\tmain(**parse_args())","metadata":{"id":"i8SAbuXEJb2A","outputId":"3808f409-19c9-426c-dc15-1b88b0c21645","trusted":true},"execution_count":null,"outputs":[]}]}